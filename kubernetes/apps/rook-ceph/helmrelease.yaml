apiVersion: helm.toolkit.fluxcd.io/v2
kind: HelmRelease
metadata:
  name: rook-ceph-cluster
  namespace: rook-ceph
spec:
  chart:
    spec:
      chart: rook-ceph-cluster
      interval: 30m
      sourceRef:
        kind: HelmRepository
        name: rook-release
        namespace: flux-system
  interval: 30m
  values:
    operatorNamespace: rook-ceph
    cephClusterSpec:
      external:
        enable: true
      crashCollector:
---
apiVersion: helm.toolkit.fluxcd.io/v2
kind: HelmRelease
metadata:
  name: rook-ceph
  namespace: rook-ceph
spec:
  chart:
    spec:
      chart: rook-ceph
      interval: 30m
      sourceRef:
        kind: HelmRepository
        name: rook-release
        namespace: flux-system
  interval: 30m
  values:
    operatorNamespace: rook-ceph
    # -- The metadata.name of the CephCluster CR
    # @default -- The same as the namespace
    clusterName:

    # -- Optional override of the target kubernetes version
    kubeVersion:

    # Installs a debugging toolbox deployment
    toolbox:
      # -- Enable Ceph debugging pod deployment. See [toolbox](../Troubleshooting/ceph-toolbox.md)
      enabled: true
      # -- Toolbox container security context
      containerSecurityContext:
        runAsNonRoot: true
        runAsUser: 2016
        runAsGroup: 2016
        capabilities:
          drop: ["ALL"]
      # -- Toolbox resources
      resources:
        limits:
          memory: "1Gi"
        requests:
          cpu: "100m"
          memory: "128Mi"
      # -- Set the priority class for the toolbox if desired
      priorityClassName:

    monitoring:
      # -- Enable Prometheus integration, will also create necessary RBAC rules to allow Operator to create ServiceMonitors.
      # Monitoring requires Prometheus to be pre-installed
      enabled: false
      # -- Whether to create the Prometheus rules for Ceph alerts
      createPrometheusRules: false
      # -- The namespace in which to create the prometheus rules, if different from the rook cluster namespace.
      # If you have multiple rook-ceph clusters in the same k8s cluster, choose the same namespace (ideally, namespace with prometheus
      # deployed) to set rulesNamespaceOverride for all the clusters. Otherwise, you will get duplicate alerts with multiple alert definitions.
      rulesNamespaceOverride:

    # -- Create & use PSP resources. Set this to the same value as the rook-ceph chart.
    pspEnable: false

    cephClusterSpec:
      external:
        enable: true

      cephVersion:
        image: quay.io/ceph/ceph:v18.2.4
        allowUnsupported: false
      dataDirHostPath: /var/lib/rook

      mgr:
        # When higher availability of the mgr is needed, increase the count to 2.
        # In that case, one mgr will be active and one in standby. When Ceph updates which
        # mgr is active, Rook will update the mgr services to match the active mgr.
        count: 2
        allowMultiplePerNode: false
        modules:
          # List of modules to optionally enable or disable.
          # Note the "dashboard" and "monitoring" modules are already configured by other settings in the cluster CR.
          # - name: rook
          #   enabled: true

      # enable the ceph dashboard for viewing cluster status
      dashboard:
        enabled: true
        # serve the dashboard under a subpath (useful when you are accessing the dashboard via a reverse proxy)
        # urlPrefix: /ceph-dashboard
        # serve the dashboard at the given port.
        # port: 8443
        # Serve the dashboard using SSL (if using ingress to expose the dashboard and `ssl: true` you need to set
        # the corresponding "backend protocol" annotation(s) for your ingress controller of choice)
        ssl: false
    ingress:
      # -- Enable an ingress for the ceph-dashboard
      dashboard:
        host:
          name: "dashboard.${SECRET_DOMAIN}"
          path: "/ceph-dashboard(/|$)(.*)"
    csiDriverNamePrefix:
